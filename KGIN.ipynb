{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbc852c-cdc2-4c48-9ebe-82b2076a5b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiotlab3/anaconda3/envs/hungvv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_normal_, xavier_uniform_\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from os.path import abspath\n",
    "import random\n",
    "import collections  \n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "from itertools import product\n",
    "from random import shuffle,randint,choice,sample\n",
    "import csv \n",
    "from tqdm import tqdm\n",
    "\n",
    "from util.conf import OptionConf\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigs\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, EmbLoss, contrastLoss\n",
    "from util.init import *\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "import os\n",
    "import numpy as np \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import networkx as nx \n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "from util.conf import ModelConf\n",
    "from base.recommender import Recommender\n",
    "from util.algorithm import find_k_largest\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "from data.loader import FileIO\n",
    "from util.evaluation import ranking_evaluation,early_stopping\n",
    "from data.data import Data\n",
    "from data.graph import Graph\n",
    "import multiprocessing\n",
    "import heapq\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from prettytable import PrettyTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba31c097-b799-4302-80aa-39dac13db851",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "cores = multiprocessing.cpu_count() // 2\n",
    "batch_test_flag = False\n",
    "test_flag= 'full'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a243349-ffeb-4ef0-a4d6-362b54059c1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Graph Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5809872-6a59-4b9c-8bcf-c23fa68963d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphRecommender(Recommender):\n",
    "    def __init__(self, conf, data, data_kg, knowledge_set, **kwargs):\n",
    "        super(GraphRecommender, self).__init__(conf, data, data_kg, knowledge_set,**kwargs)\n",
    "        self.data = data\n",
    "        self.data_kg = data_kg\n",
    "        self.bestPerformance = []\n",
    "        top = self.ranking['-topN'].split(',')\n",
    "        self.Ks = [int(num) for num in top]\n",
    "        self.max_N = max(self.Ks)\n",
    "        \n",
    "        self.output_path = kwargs['output_path']\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "            \n",
    "    def print_model_info(self):\n",
    "        super(GraphRecommender, self).print_model_info()\n",
    "        # # print dataset statistics\n",
    "        print('Training Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.training_size()))\n",
    "        print('Test Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.test_size()))\n",
    "        print('=' * 80)\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, u):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def test(self, user_emb, item_emb):\n",
    "        def process_bar(num, total):\n",
    "            rate = float(num) / total\n",
    "            ratenum = int(50 * rate)\n",
    "            r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "            sys.stdout.write(r)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # predict\n",
    "        rec_list = {}\n",
    "        user_count = len(self.data.test_set)\n",
    "        lst_users =  list(self.data_kg.u2id.keys())\n",
    "        lst_items =  list(self.data_kg.i2id.keys())\n",
    "        \n",
    "        for i, user in enumerate(self.data.test_set):\n",
    "            user_id  = lst_users.index(user)\n",
    "            score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "            candidates = score.cpu().numpy()\n",
    "            \n",
    "            # e_find_candidates = time.time()\n",
    "            # print(\"Calculate candidates time: %f s\" % (e_find_candidates - s_find_candidates))\n",
    "            # predictedItems = denormalize(predictedItems, self.data.rScale[-1], self.data.rScale[0])\n",
    "            rated_list, li = self.data.user_rated(user)\n",
    "            for item in rated_list:\n",
    "                candidates[lst_items.index(item)] = -10e8\n",
    "            # s_find_k_largest = time.time()\n",
    "            ids, scores = find_k_largest(self.max_N, candidates)\n",
    "            # e_find_k_largest = time.time()\n",
    "            # print(\"Find k largest candidates: %f s\" % (e_find_k_largest - s_find_k_largest))\n",
    "            item_names = [lst_items[iid] for iid in ids]\n",
    "            rec_list[user] = list(zip(item_names, scores))\n",
    "            if i % 1000 == 0:\n",
    "                process_bar(i, user_count)\n",
    "        process_bar(user_count, user_count)\n",
    "        print('')\n",
    "        return rec_list\n",
    "    \n",
    "    def evaluate(self, rec_list):\n",
    "        self.recOutput.append('userId: recommendations in (itemId, ranking score) pairs, * means the item is hit.\\n')\n",
    "        for user in self.data.test_set:\n",
    "            line = str(user) + ':'\n",
    "            for item in rec_list[user]:\n",
    "                line += ' (' + str(item[0]) + ',' + str(item[1]) + ')'\n",
    "                if item[0] in self.data.test_set[user]:\n",
    "                    line += '*'\n",
    "            line += '\\n'\n",
    "            self.recOutput.append(line)\n",
    "        current_time = strftime(\"%Y-%m-%d %H-%M-%S\", localtime(time.time()))\n",
    "        # output prediction result\n",
    "        out_dir = self.output_path\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-top-' + str(self.max_N) + 'items' + '.txt'\n",
    "        FileIO.write_file(out_dir, file_name, self.recOutput)\n",
    "        print('The result has been output to ', abspath(out_dir), '.')\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-performance' + '.txt'\n",
    "        self.result = ranking_evaluation(self.data.test_set, rec_list, self.topN)\n",
    "        self.model_log.add('###Evaluation Results###')\n",
    "        self.model_log.add(self.result)\n",
    "        FileIO.write_file(out_dir, file_name, self.result)\n",
    "        print('The result of %s:\\n%s' % (self.model_name, ''.join(self.result)))\n",
    "\n",
    "    def fast_evaluation(self, model, epoch, user_embed, item_embed, kwargs=None):\n",
    "        print('Evaluating the model...')\n",
    "        s_test = time.time()\n",
    "        rec_list = self.test(user_embed, item_embed)\n",
    "        e_test = time.time() \n",
    "        print(\"Test time: %f s\" % (e_test - s_test))\n",
    "        \n",
    "        s_measure = time.time()\n",
    "        measure = ranking_evaluation(self.data.test_set, rec_list, [self.max_N])\n",
    "        e_measure = time.time()\n",
    "        print(\"Measure time: %f s\" % (e_measure - s_measure))\n",
    "        \n",
    "        if len(self.bestPerformance) > 0:\n",
    "            count = 0\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            for k in self.bestPerformance[1]:\n",
    "                if self.bestPerformance[1][k] > performance[k]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count -= 1\n",
    "            if count < 0:\n",
    "                self.bestPerformance[1] = performance\n",
    "                self.bestPerformance[0] = epoch + 1\n",
    "                # try:\n",
    "                #     self.save(kwargs)\n",
    "                # except:\n",
    "                self.save(model)\n",
    "        else:\n",
    "            self.bestPerformance.append(epoch + 1)\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            self.bestPerformance.append(performance)\n",
    "            # try:\n",
    "            #     self.save(kwargs)\n",
    "            # except:\n",
    "            self.save(model)\n",
    "        print('-' * 120)\n",
    "        print('Real-Time Ranking Performance ' + ' (Top-' + str(self.max_N) + ' Item Recommendation)')\n",
    "        measure = [m.strip() for m in measure[1:]]\n",
    "        print('*Current Performance*')\n",
    "        print('Epoch:', str(epoch + 1) + ',', '  |  '.join(measure))\n",
    "        bp = ''\n",
    "        # for k in self.bestPerformance[1]:\n",
    "        #     bp+=k+':'+str(self.bestPerformance[1][k])+' | '\n",
    "        bp += 'Hit Ratio' + ':' + str(self.bestPerformance[1]['Hit Ratio']) + '  |  '\n",
    "        bp += 'Precision' + ':' + str(self.bestPerformance[1]['Precision']) + '  |  '\n",
    "        bp += 'Recall' + ':' + str(self.bestPerformance[1]['Recall']) + '  |  '\n",
    "        # bp += 'F1' + ':' + str(self.bestPerformance[1]['F1']) + ' | '\n",
    "        bp += 'NDCG' + ':' + str(self.bestPerformance[1]['NDCG'])\n",
    "        print('*Best Performance* ')\n",
    "        print('Epoch:fast_evaluation', str(self.bestPerformance[0]) + ',', bp)\n",
    "        print('-' * 120)\n",
    "        return measure\n",
    "    \n",
    "    def save(self, model):\n",
    "        with torch.no_grad():\n",
    "            user_emb, entity_emb = model.calc_cf_embeddings()\n",
    "            item_emb = entity_emb[list(model.data_kg.id2i.keys())]\n",
    "            self.best_user_emb, self.best_item_emb = user_emb, item_emb\n",
    "        self.save_model(model)\n",
    "    \n",
    "    def save_model(self, model):\n",
    "        # save model \n",
    "        current_time = strftime(\"%Y-%m-%d\", localtime(time.time()))\n",
    "        out_dir = self.output_path\n",
    "        file_name =  self.config['model.name'] + '@' + current_time + '-weight' + '.pth'\n",
    "        weight_file = out_dir + '/' + file_name \n",
    "        torch.save(model.state_dict(), weight_file)\n",
    "\n",
    "    def save_performance_row(self, ep, data_ep):\n",
    "        # opening the csv file in 'w' mode\n",
    "        csv_path = self.output_path + 'train_performance.csv'\n",
    "        \n",
    "        # 'Hit Ratio:0.00328', 'Precision:0.00202', 'Recall:0.00337', 'NDCG:0.00292\n",
    "        hit = float(data_ep[0].split(':')[1])\n",
    "        precision = float(data_ep[1].split(':')[1])\n",
    "        recall = float(data_ep[2].split(':')[1])\n",
    "        ndcg = float(data_ep[3].split(':')[1])\n",
    "        \n",
    "        with open(csv_path, 'a+', newline = '') as f:\n",
    "            header = ['ep', 'hit@20', 'prec@20', 'recall@20', 'ndcg@20']\n",
    "            writer = csv.DictWriter(f, fieldnames = header)\n",
    "            # writer.writeheader()\n",
    "            writer.writerow({\n",
    "                 'ep' : ep,\n",
    "                 'hit@20': hit,\n",
    "                 'prec@20': precision,\n",
    "                 'recall@20': recall,\n",
    "                 'ndcg@20': ndcg,\n",
    "            })\n",
    "            \n",
    "    def save_loss_row(self, data_ep):\n",
    "        csv_path = self.output_path + 'loss.csv'\n",
    "        with open(csv_path, 'a+', newline ='') as f:\n",
    "            header = ['ep', 'train_loss', 'cf_loss', 'kg_loss']\n",
    "            writer = csv.DictWriter(f, fieldnames = header)\n",
    "            # writer.writeheader()\n",
    "            writer.writerow({\n",
    "                'ep' : data_ep[0],\n",
    "                'train_loss': data_ep[1],\n",
    "                 'cf_loss': data_ep[2],\n",
    "                 'kg_loss': data_ep[3]\n",
    "            })\n",
    "\n",
    "    def save_loss(self, train_losses, rec_losses, kg_losses):\n",
    "        df_train_loss = pd.DataFrame(train_losses, columns = ['ep', 'loss'])\n",
    "        df_rec_loss = pd.DataFrame(rec_losses, columns = ['ep', 'loss'])\n",
    "        df_kg_loss = pd.DataFrame(kg_losses, columns = ['ep', 'loss'])\n",
    "        df_train_loss.to_csv(self.output_path + '/train_loss.csv')\n",
    "        df_rec_loss.to_csv(self.output_path + '/rec_loss.csv')\n",
    "        df_kg_loss.to_csv(self.output_path + '/kg_loss.csv')\n",
    "    \n",
    "    def save_perfomance_training(self, log_train):\n",
    "        df_train_log = pd.DataFrame(log_train)\n",
    "        df_train_log.to_csv(self.output_path + '/train_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daafda-3820-49bb-b862-c76b720d0b32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0c5817-6093-4b16-8dc7-652683f7dbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Interaction(Data, Graph):\n",
    "    def __init__(self, conf, training, test):\n",
    "        self.conf = conf \n",
    "        Graph.__init__(self)\n",
    "        Data.__init__(self,conf,training,test)\n",
    "\n",
    "        self.user = {}\n",
    "        self.item = {}\n",
    "        self.id2user = {}\n",
    "        self.id2item = {}\n",
    "\n",
    "        self.training_set_u = defaultdict(dict)\n",
    "        self.training_set_i = defaultdict(dict)\n",
    "        self.test_set = defaultdict(dict)\n",
    "        self.user_history_dict = defaultdict(dict)\n",
    "\n",
    "        self.test_set_item = set()\n",
    "        self.__generate_set()\n",
    "\n",
    "        self.n_users = len(self.training_set_u)\n",
    "        self.n_items = len(self.training_set_i) \n",
    "\n",
    "        self.n_cf_train = len(self.training_data)\n",
    "        self.n_cf_test = len(self.test_data)\n",
    "        \n",
    "    def __generate_set(self):\n",
    "        for entry in self.training_data:\n",
    "            user, item, rating = entry\n",
    "            user, item = int(user), int(item)\n",
    "            if user not in self.user:\n",
    "                self.user[user] = len(self.user)\n",
    "                self.id2user[self.user[user]] = user\n",
    "            if item not in self.item:\n",
    "                self.item[item] = len(self.item)\n",
    "                self.id2item[self.item[item]] = item\n",
    "                # userList.append\n",
    "            # construct user_history_dict \n",
    "            if rating == 1.0:\n",
    "                if user not in self.user_history_dict:\n",
    "                    self.user_history_dict[user] = []\n",
    "                self.user_history_dict[user].append(item)\n",
    "\n",
    "            self.training_set_u[user][item] = rating\n",
    "            self.training_set_i[item][user] = rating\n",
    "        \n",
    "        for entry in self.test_data:\n",
    "            user, item, rating = entry\n",
    "            if user not in self.user:\n",
    "                continue\n",
    "            self.test_set[user][item] = rating\n",
    "            self.test_set_item.add(item)\n",
    "\n",
    "    def __create_sparse_bipartite_adjacency(self, self_connection=False):\n",
    "        '''\n",
    "        return a sparse adjacency matrix with the shape (user number + item number, user number + item number)\n",
    "        '''\n",
    "        n_nodes = self.n_users + self.n_items\n",
    "        row_idx = [int(pair[0]) for pair in self.training_data]\n",
    "        col_idx = [int(pair[1]) for pair in self.training_data]\n",
    "        user_np = np.array(row_idx)\n",
    "        item_np = np.array(col_idx)\n",
    "        ratings = np.ones_like(user_np, dtype=np.float32)\n",
    "        tmp_adj = sp.csr_matrix((ratings, (user_np, item_np + self.n_users)), shape=(n_nodes, n_nodes),dtype=np.float32)\n",
    "        adj_mat = tmp_adj + tmp_adj.T\n",
    "        if self_connection:\n",
    "            adj_mat += sp.eye(n_nodes)\n",
    "        return adj_mat\n",
    "    \n",
    "    def __create_sparse_interaction_matrix(self):\n",
    "        \"\"\"\n",
    "            return a sparse adjacency matrix with the shape (user number, item number)\n",
    "        \"\"\"\n",
    "        row, col, entries = [], [], []\n",
    "        for pair in self.training_data:\n",
    "            row += [int(pair[0])]\n",
    "            col += [int(pair[1])]\n",
    "            entries += [1.0]\n",
    "        interaction_mat = sp.csr_matrix((entries, (row, col)), shape=(self.n_users,self.n_items),dtype=np.float32)\n",
    "        inv_interaction_mat = sp.csr_matrix((entries, (col, row)), shape=(self.n_items, self.n_users), dtype=np.float32)\n",
    "        return interaction_mat, inv_interaction_mat\n",
    "            \n",
    "    def get_user_id(self, u):\n",
    "        if u in self.user:\n",
    "            return self.user[u]\n",
    "\n",
    "    def get_item_id(self, i):\n",
    "        if i in self.item:\n",
    "            return self.item[i]\n",
    "\n",
    "    def training_size(self):\n",
    "        return len(self.user), len(self.item), len(self.training_data)\n",
    "\n",
    "    def test_size(self):\n",
    "        return len(self.test_set), len(self.test_set_item), len(self.test_data)\n",
    "\n",
    "    def contain(self, u, i):\n",
    "        'whether user u rated item i'\n",
    "        if u in self.user and i in self.training_set_u[u]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def contain_user(self, u):\n",
    "        'whether user is in training set'\n",
    "        if u in self.user:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def contain_item(self, i):\n",
    "        \"\"\"whether item is in training set\"\"\"\n",
    "        if i in self.item:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def user_rated(self, u):\n",
    "        return list(self.training_set_u[u].keys()), list(self.training_set_u[u].values())\n",
    "\n",
    "    def item_rated(self, i):\n",
    "        return list(self.training_set_i[i].keys()), list(self.training_set_i[i].values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4bb4a-e750-4360-801b-4a90b4f06bec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193ecb06-7fc5-405b-a3e4-cd8063fafc64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Knowledge:\n",
    "    def __init__(self, conf, training, test, knowledge, inverse_r=True):\n",
    "        self.conf = conf \n",
    "\n",
    "        self.inverse_r = inverse_r\n",
    "        self.kg_data = knowledge\n",
    "\n",
    "        self.relation = {}\n",
    "        self.id2rel = {}\n",
    "        \n",
    "        self.cf_train_data = np.array(training)\n",
    "        self.cf_test_data = np.array(test)\n",
    "        \n",
    "        self.training_set_e = defaultdict(dict)\n",
    "        self.train_user_set = defaultdict(list)\n",
    "        self.test_user_set = defaultdict(list)\n",
    "\n",
    "        self.u2id = {}\n",
    "        self.id2u = {}\n",
    "        self.i2id = {}\n",
    "        self.id2i = {}\n",
    "        \n",
    "        self.remap_item()\n",
    "        self.user_dict = {\n",
    "            'train_user_set': self.train_user_set,\n",
    "            'test_user_set': self.test_user_set\n",
    "        }\n",
    "        self.triplets = self.read_triplets()\n",
    "\n",
    "        # get data \n",
    "        self.graph, relation_dict = self.build_graph()\n",
    "        self.adj_mat, self.norm_mat_list, self.mean_mat_list = self.__create_sparse_relational_graph(relation_dict)\n",
    "        # self.kg_interaction_mat = self.__create_sparse_knowledge_interaction_matrix()\n",
    "\n",
    "    def remap_item(self):\n",
    "        min_user = min(min(self.cf_train_data[:,0]), min(self.cf_test_data[:,0]))\n",
    "        self.n_users =  int(max(max(self.cf_train_data[:,0]), max(self.cf_test_data[:,0])) - min(min(self.cf_train_data[:,0]), min(self.cf_test_data[:,0])) + 2)\n",
    "        \n",
    "        for u_id, i_id, _ in self.cf_train_data:\n",
    "            self.train_user_set[int(u_id)].append(int(i_id))\n",
    "            if int(u_id) not in self.u2id:\n",
    "                self.u2id[int(u_id)] = int(u_id- min_user) \n",
    "                self.id2u[int(u_id- min_user)] = int(u_id)\n",
    "            if int(i_id) not in self.i2id:\n",
    "                self.i2id[int(i_id)] = len(self.i2id)\n",
    "                self.id2i[self.i2id[int(i_id)]] = int(i_id)\n",
    "            \n",
    "        for u_id, i_id, _ in self.cf_test_data:\n",
    "            self.test_user_set[int(u_id)].append(int(i_id))\n",
    "            if int(u_id) not in self.u2id:\n",
    "                self.u2id[int(u_id)] = int(u_id- min_user) \n",
    "                self.id2u[int(u_id- min_user)] = int(u_id)\n",
    "            if int(i_id) not in self.i2id:\n",
    "                self.i2id[int(i_id)] = len(self.i2id)\n",
    "                self.id2i[self.i2id[int(i_id)]] = int(i_id)\n",
    "        self.n_items = len(self.i2id)\n",
    "                \n",
    "    def read_triplets(self):\n",
    "        can_triplets_np = self.kg_data.to_numpy(dtype=np.int32)\n",
    "        can_triplets_np = np.unique(can_triplets_np, axis=0)\n",
    "    \n",
    "        if self.inverse_r:\n",
    "            # get triplets with inverse direction like <entity, is-aspect-of, item>\n",
    "            inv_triplets_np = can_triplets_np.copy()\n",
    "            inv_triplets_np[:, 0] = can_triplets_np[:, 2]\n",
    "            inv_triplets_np[:, 2] = can_triplets_np[:, 0]\n",
    "            inv_triplets_np[:, 1] = can_triplets_np[:, 1] + max(can_triplets_np[:, 1]) + 1\n",
    "            # consider two additional relations --- 'interact' and 'be interacted'\n",
    "            can_triplets_np[:, 1] = can_triplets_np[:, 1] + 1\n",
    "            inv_triplets_np[:, 1] = inv_triplets_np[:, 1] + 1\n",
    "            # get full version of knowledge graph\n",
    "            triplets = np.concatenate((can_triplets_np, inv_triplets_np), axis=0)\n",
    "        else:\n",
    "            # consider two additional relations --- 'interact'.\n",
    "            can_triplets_np[:, 1] = can_triplets_np[:, 1] + 1\n",
    "            triplets = can_triplets_np.copy()\n",
    "        self.n_entities = int(max(max(triplets[:, 0]), max(triplets[:, 2]))) + 1  # including items + users\n",
    "        self.n_nodes = int(self.n_entities + self.n_users) \n",
    "        self.n_relations = int(max(triplets[:, 1]) + 1)\n",
    "        return triplets\n",
    "        \n",
    "    def build_graph(self):\n",
    "        print(\"Building the graph ...\")\n",
    "        ckg_graph = nx.MultiDiGraph()\n",
    "        rd = collections.defaultdict(list)\n",
    "\n",
    "        print(\"Begin to load interaction triplets\")\n",
    "        for u_id, i_id, _ in tqdm(self.cf_train_data, ascii=True):\n",
    "            rd[0].append([u_id, i_id])\n",
    "            \n",
    "        print(\"Begin to load knowledge triplets\")\n",
    "        for h_id, r_id, t_id in tqdm(self.triplets, ascii=True):\n",
    "            ckg_graph.add_edge(h_id, t_id, key=r_id)\n",
    "            rd[r_id].append([h_id, t_id])\n",
    "        return ckg_graph, rd\n",
    "\n",
    "    def __create_sparse_relational_graph(self, relation_dict):\n",
    "        def _bi_norm_lap(adj):\n",
    "            # D^{-1/2}AD^{-1/2}\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "    \n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    \n",
    "            # bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "    \n",
    "        def _si_norm_lap(adj):\n",
    "            # D^{-1}A\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "    \n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "    \n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "    \n",
    "        adj_mat_list = []\n",
    "        print(\"Begin to build sparse relation matrix ...\")\n",
    "        for r_id in tqdm(relation_dict.keys()):\n",
    "            np_mat = np.array(relation_dict[r_id])\n",
    "            if r_id == 0:\n",
    "                cf = np_mat.copy()\n",
    "                # cf[:, 1] = cf[:, 1] + self.n_users  # [0, n_items) -> [n_users, n_users+n_items)\n",
    "                vals = [1.] * len(cf)\n",
    "                adj = sp.coo_matrix((vals, (cf[:, 0], cf[:, 1])), shape=(self.n_nodes, self.n_nodes))\n",
    "            else:\n",
    "                vals = [1.] * len(np_mat)\n",
    "                adj = sp.coo_matrix((vals, (np_mat[:, 0], np_mat[:, 1])), shape=(self.n_nodes, self.n_nodes))\n",
    "            adj_mat_list.append(adj)\n",
    "    \n",
    "        norm_mat_list = [_bi_norm_lap(mat) for mat in adj_mat_list]\n",
    "        mean_mat_list = [_si_norm_lap(mat) for mat in adj_mat_list]\n",
    "        # interaction: user->item, [n_users, n_entities]\n",
    "        norm_mat_list[0] = norm_mat_list[0].tocsr()[self.n_entities:, :self.n_entities].tocoo()\n",
    "        mean_mat_list[0] = mean_mat_list[0].tocsr()[self.n_entities:, :self.n_entities].tocoo()\n",
    "        return adj_mat_list, norm_mat_list, mean_mat_list \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c681d9-7a1a-4092-8738-2fef044c381c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030db1cb-41fc-4618-b2ae-b9a56466e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Relational Path-aware Convolution Network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_factors):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_factors = n_factors\n",
    "\n",
    "    def forward(self, entity_emb, user_emb, latent_emb,\n",
    "                edge_index, edge_type, interact_mat,\n",
    "                weight, disen_weight_att):\n",
    "\n",
    "        n_entities = entity_emb.shape[0]\n",
    "        channel = entity_emb.shape[1]\n",
    "        n_users = self.n_users\n",
    "        n_factors = self.n_factors\n",
    "\n",
    "        \"\"\"KG aggregate\"\"\"\n",
    "        head, tail = edge_index\n",
    "        head = head.to(device)\n",
    "        tail = tail.to(device)\n",
    "        entity_emb = entity_emb.to(device)\n",
    "        \n",
    "        edge_relation_emb = weight[edge_type - 1].to(device)  # exclude interact, remap [1, n_relations) to [0, n_relations-1)\n",
    "        neigh_relation_emb = entity_emb[tail] * edge_relation_emb  # [-1, channel]\n",
    "        entity_agg = scatter_mean(src=neigh_relation_emb, index=head, dim_size=n_entities, dim=0)\n",
    "        \"\"\"cul user->latent factor attention\"\"\"\n",
    "        score_ = torch.mm(user_emb, latent_emb.t())\n",
    "        score = nn.Softmax(dim=1)(score_).unsqueeze(-1).to(device)  # [n_users, n_factors, 1]\n",
    "\n",
    "        \"\"\"user aggregate\"\"\"\n",
    "        user_agg = torch.sparse.mm(interact_mat, entity_emb).to(device)  # [n_users, channel]\n",
    "        disen_weight = torch.mm(nn.Softmax(dim=-1)(disen_weight_att), weight).expand(int(n_users), n_factors, channel).to(device)\n",
    "        user_agg = user_agg * (disen_weight * score).sum(dim=1) + user_agg  # [n_users, channel]\n",
    "\n",
    "        return entity_agg, user_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0089c4ea-ce4e-457f-a914-60cef1c380b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, n_hops, n_users,\n",
    "                 n_factors, n_relations, interact_mat,\n",
    "                 ind, node_dropout_rate=0.5, mess_dropout_rate=0.1):\n",
    "        super(GraphConv, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList().to(device)\n",
    "        self.interact_mat = interact_mat\n",
    "        self.n_relations = n_relations\n",
    "        self.n_users = n_users\n",
    "        self.n_factors = n_factors\n",
    "        self.node_dropout_rate = node_dropout_rate\n",
    "        self.mess_dropout_rate = mess_dropout_rate\n",
    "        self.ind = ind\n",
    "\n",
    "        self.temperature = 0.2\n",
    "\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        weight = initializer(torch.empty(n_relations - 1, channel))  # not include interact\n",
    "        self.weight = nn.Parameter(weight).to(device)  # [n_relations - 1, in_channel]\n",
    "\n",
    "        disen_weight_att = initializer(torch.empty(n_factors, n_relations - 1))\n",
    "        self.disen_weight_att = nn.Parameter(disen_weight_att).to(device)\n",
    "\n",
    "        for i in range(n_hops):\n",
    "            self.convs.append(Aggregator(n_users=n_users, n_factors=n_factors).to(device))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=mess_dropout_rate)  # mess dropout\n",
    "\n",
    "    def _edge_sampling(self, edge_index, edge_type, rate=0.5):\n",
    "        # edge_index: [2, -1]\n",
    "        # edge_type: [-1]\n",
    "        n_edges = edge_index.shape[1]\n",
    "        random_indices = np.random.choice(n_edges, size=int(n_edges * rate), replace=False)\n",
    "        return edge_index[:, random_indices], edge_type[random_indices]\n",
    "\n",
    "    def _sparse_dropout(self, x, rate=0.5):\n",
    "        noise_shape = x._nnz()\n",
    "\n",
    "        random_tensor = rate\n",
    "        random_tensor += torch.rand(noise_shape).to(x.device)\n",
    "        dropout_mask = torch.floor(random_tensor).type(torch.bool)\n",
    "        i = x._indices()\n",
    "        v = x._values()\n",
    "\n",
    "        i = i[:, dropout_mask]\n",
    "        v = v[dropout_mask]\n",
    "\n",
    "        out = torch.sparse.FloatTensor(i, v, x.shape).to(x.device)\n",
    "        return out * (1. / (1 - rate))\n",
    "        \n",
    "    def _cul_cor(self):\n",
    "        def CosineSimilarity(tensor_1, tensor_2):\n",
    "            # tensor_1, tensor_2: [channel]\n",
    "            normalized_tensor_1 = tensor_1 / tensor_1.norm(dim=0, keepdim=True)\n",
    "            normalized_tensor_2 = tensor_2 / tensor_2.norm(dim=0, keepdim=True)\n",
    "            return (normalized_tensor_1 * normalized_tensor_2).sum(dim=0) ** 2  # no negative\n",
    "        def DistanceCorrelation(tensor_1, tensor_2):\n",
    "            # tensor_1, tensor_2: [channel]\n",
    "            # ref: https://en.wikipedia.org/wiki/Distance_correlation\n",
    "            channel = tensor_1.shape[0]\n",
    "            zeros = torch.zeros(channel, channel).to(tensor_1.device)\n",
    "            zero = torch.zeros(1).to(tensor_1.device)\n",
    "            tensor_1, tensor_2 = tensor_1.unsqueeze(-1), tensor_2.unsqueeze(-1)\n",
    "            \"\"\"cul distance matrix\"\"\"\n",
    "            a_, b_ = torch.matmul(tensor_1, tensor_1.t()) * 2, \\\n",
    "                   torch.matmul(tensor_2, tensor_2.t()) * 2  # [channel, channel]\n",
    "            tensor_1_square, tensor_2_square = tensor_1 ** 2, tensor_2 ** 2\n",
    "            a, b = torch.sqrt(torch.max(tensor_1_square - a_ + tensor_1_square.t(), zeros) + 1e-8), \\\n",
    "                   torch.sqrt(torch.max(tensor_2_square - b_ + tensor_2_square.t(), zeros) + 1e-8)  # [channel, channel]\n",
    "            \"\"\"cul distance correlation\"\"\"\n",
    "            A = a - a.mean(dim=0, keepdim=True) - a.mean(dim=1, keepdim=True) + a.mean()\n",
    "            B = b - b.mean(dim=0, keepdim=True) - b.mean(dim=1, keepdim=True) + b.mean()\n",
    "            dcov_AB = torch.sqrt(torch.max((A * B).sum() / channel ** 2, zero) + 1e-8)\n",
    "            dcov_AA = torch.sqrt(torch.max((A * A).sum() / channel ** 2, zero) + 1e-8)\n",
    "            dcov_BB = torch.sqrt(torch.max((B * B).sum() / channel ** 2, zero) + 1e-8)\n",
    "            return dcov_AB / torch.sqrt(dcov_AA * dcov_BB + 1e-8)\n",
    "            \n",
    "        def MutualInformation():\n",
    "            # disen_T: [num_factor, dimension]\n",
    "            disen_T = self.disen_weight_att.t()\n",
    "\n",
    "            # normalized_disen_T: [num_factor, dimension]\n",
    "            normalized_disen_T = disen_T / disen_T.norm(dim=1, keepdim=True)\n",
    "\n",
    "            pos_scores = torch.sum(normalized_disen_T * normalized_disen_T, dim=1)\n",
    "            ttl_scores = torch.sum(torch.mm(disen_T, self.disen_weight_att), dim=1)\n",
    "\n",
    "            pos_scores = torch.exp(pos_scores / self.temperature)\n",
    "            ttl_scores = torch.exp(ttl_scores / self.temperature)\n",
    "\n",
    "            mi_score = - torch.sum(torch.log(pos_scores / ttl_scores))\n",
    "            return mi_score\n",
    "\n",
    "        \"\"\"cul similarity for each latent factor weight pairs\"\"\"\n",
    "        if self.ind == 'mi':\n",
    "            return MutualInformation()\n",
    "        else:\n",
    "            cor = 0\n",
    "            for i in range(self.n_factors):\n",
    "                for j in range(i + 1, self.n_factors):\n",
    "                    if self.ind == 'distance':\n",
    "                        cor += DistanceCorrelation(self.disen_weight_att[i], self.disen_weight_att[j])\n",
    "                    else:\n",
    "                        cor += CosineSimilarity(self.disen_weight_att[i], self.disen_weight_att[j])\n",
    "        return cor\n",
    "\n",
    "    def forward(self, user_emb, entity_emb, latent_emb, edge_index, edge_type,\n",
    "                interact_mat, mess_dropout=True, node_dropout=False):\n",
    "\n",
    "        \"\"\"node dropout\"\"\"\n",
    "        if node_dropout:\n",
    "            edge_index, edge_type = self._edge_sampling(edge_index, edge_type, self.node_dropout_rate)\n",
    "            interact_mat = self._sparse_dropout(interact_mat, self.node_dropout_rate)\n",
    "\n",
    "        entity_res_emb = entity_emb  # [n_entity, channel]\n",
    "        user_res_emb = user_emb  # [n_users, channel]\n",
    "        cor = self._cul_cor()\n",
    "        for i in range(len(self.convs)):\n",
    "            entity_emb, user_emb = self.convs[i](entity_emb, user_emb, latent_emb,\n",
    "                                                 edge_index, edge_type, interact_mat,\n",
    "                                                 self.weight, self.disen_weight_att)\n",
    "\n",
    "            \"\"\"message dropout\"\"\"\n",
    "            if mess_dropout:\n",
    "                entity_emb = self.dropout(entity_emb)\n",
    "                user_emb = self.dropout(user_emb)\n",
    "            entity_emb = F.normalize(entity_emb)\n",
    "            user_emb = F.normalize(user_emb)\n",
    "\n",
    "            \"\"\"result emb\"\"\"\n",
    "            entity_res_emb = torch.add(entity_res_emb, entity_emb)\n",
    "            user_res_emb = torch.add(user_res_emb, user_emb)\n",
    "\n",
    "        return entity_res_emb, user_res_emb, cor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216710e-2725-450e-a2e8-116c959bc8f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ac61ab-4ae5-4708-8a7c-f6e1baa0c7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KGIN(nn.Module):\n",
    "    def __init__(self, args, rec, device=None):\n",
    "        super(KGIN, self).__init__()\n",
    "\n",
    "        self.data_kg = rec.data_kg \n",
    "        self.n_users = self.data_kg.n_users\n",
    "        self.n_relations = self.data_kg.n_relations\n",
    "        self.n_entities = self.data_kg.n_entities  # include items\n",
    "        self.n_nodes = rec.data_kg.n_nodes # n_users + n_entities\n",
    "\n",
    "        self.decay = args['l2']\n",
    "        self.sim_decay = args['sim_regularity']\n",
    "        self.emb_size = args['dim']\n",
    "        self.context_hops = args['context_hops']\n",
    "        self.n_factors = args['n_factors']\n",
    "        self.node_dropout = args['node_dropout']\n",
    "        self.node_dropout_rate = args['node_dropout_rate']\n",
    "        self.mess_dropout = args['mess_dropout']\n",
    "        self.mess_dropout_rate = args['mess_dropout_rate']\n",
    "        self.ind = args['ind']\n",
    "        self.device = device\n",
    "\n",
    "        self.adj_mat = self.data_kg.mean_mat_list[0]\n",
    "        self.graph = self.data_kg.graph\n",
    "        self.edge_index, self.edge_type = self._get_edges(self.graph)\n",
    "\n",
    "        self._init_weight()\n",
    "        self.all_embed = nn.Parameter(self.all_embed).to(self.device)\n",
    "        self.latent_emb = nn.Parameter(self.latent_emb).to(self.device)\n",
    "\n",
    "        self.gcn = self._init_model()\n",
    "\n",
    "    def _init_weight(self):\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        self.all_embed = initializer(torch.empty(self.n_nodes, self.emb_size).to(self.device))\n",
    "        self.latent_emb = initializer(torch.empty(self.n_factors, self.emb_size).to(self.device))\n",
    "\n",
    "        # [n_users, n_entities]\n",
    "        self.interact_mat = self._convert_sp_mat_to_sp_tensor(self.adj_mat).to(self.device)\n",
    "\n",
    "    def _init_model(self):\n",
    "        return GraphConv(channel=self.emb_size,\n",
    "                         n_hops=self.context_hops,\n",
    "                         n_users=self.n_users,\n",
    "                         n_relations=self.n_relations,\n",
    "                         n_factors=self.n_factors,\n",
    "                         interact_mat=self.interact_mat,\n",
    "                         ind=self.ind,\n",
    "                         node_dropout_rate=self.node_dropout_rate,\n",
    "                         mess_dropout_rate=self.mess_dropout_rate)\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo()\n",
    "        i = torch.LongTensor([coo.row, coo.col])\n",
    "        v = torch.from_numpy(coo.data).float()\n",
    "        return torch.sparse.FloatTensor(i, v, coo.shape)\n",
    "\n",
    "    def _get_indices(self, X):\n",
    "        coo = X.tocoo()\n",
    "        return torch.LongTensor([coo.row, coo.col]).t()  # [-1, 2]\n",
    "\n",
    "    def _get_edges(self, graph):\n",
    "        graph_tensor = torch.tensor(list(graph.edges))  # [-1, 3]\n",
    "        index = graph_tensor[:, :-1]  # [-1, 2]\n",
    "        type = graph_tensor[:, -1]  # [-1, 1]\n",
    "        return index.t().long().to(self.device), type.long().to(self.device)\n",
    "\n",
    "    def forward(self, batch=None):\n",
    "        user =  torch.LongTensor([ rec.data_kg.u2id[u.item()] for u in  batch['users'] ] ).to(device)\n",
    "        pos_item = batch['pos_items']\n",
    "        neg_item = batch['neg_items']\n",
    "        user_emb = self.all_embed[self.n_entities:, :]\n",
    "        item_emb = self.all_embed[:self.n_entities, :]\n",
    "        # entity_gcn_emb: [n_entity, channel]\n",
    "        # user_gcn_emb: [n_users, channel]\n",
    "        entity_gcn_emb, user_gcn_emb, cor =  self.gcn(user_emb,\n",
    "                                                     item_emb,\n",
    "                                                     self.latent_emb,\n",
    "                                                     self.edge_index,\n",
    "                                                     self.edge_type,\n",
    "                                                     self.interact_mat,\n",
    "                                                     mess_dropout=self.mess_dropout,\n",
    "                                                     node_dropout=self.node_dropout)\n",
    "        u_e = user_gcn_emb[user]\n",
    "        pos_e, neg_e = entity_gcn_emb[pos_item], entity_gcn_emb[neg_item]\n",
    "        return self.create_bpr_loss(u_e, pos_e, neg_e, cor)\n",
    "\n",
    "    def generate(self):\n",
    "        user_emb = self.all_embed[self.n_entities:, :]\n",
    "        item_emb = self.all_embed[:self.n_entities, :]\n",
    "        return self.gcn(user_emb,\n",
    "                        item_emb,\n",
    "                        self.latent_emb,\n",
    "                        self.edge_index,\n",
    "                        self.edge_type,\n",
    "                        self.interact_mat,\n",
    "                        mess_dropout=False, node_dropout=False)[:-1]\n",
    "\n",
    "    def rating(self, u_g_embeddings, i_g_embeddings):\n",
    "        return torch.matmul(u_g_embeddings, i_g_embeddings.t())\n",
    "\n",
    "    def create_bpr_loss(self, users, pos_items, neg_items, cor):\n",
    "        batch_size = users.shape[0]\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "\n",
    "        mf_loss = -1 * torch.mean(nn.LogSigmoid()(pos_scores - neg_scores))\n",
    "\n",
    "        # cul regularizer\n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        emb_loss = self.decay * regularizer / batch_size\n",
    "        cor_loss = self.sim_decay * cor\n",
    "\n",
    "        return mf_loss + emb_loss + cor_loss, mf_loss, emb_loss, cor\n",
    "\n",
    "    def generate_kg_drop(self):\n",
    "        user_emb = self.all_embed[self.n_entities:, :]\n",
    "        item_emb = self.all_embed[:self.n_entities, :]\n",
    "        edge_index, edge_type = self.gcn._edge_sampling(self.edge_index, self.edge_type, self.kg_drop_test_keep_rate)\n",
    "        return self.gcn(user_emb,\n",
    "                        item_emb,\n",
    "                        self.latent_emb,\n",
    "                        edge_index,\n",
    "                        edge_type,\n",
    "                        self.interact_mat,\n",
    "                        mess_dropout=False, node_dropout=False)[:-1]\n",
    "\n",
    "    def calc_cf_embeddings(self):\n",
    "        user_emb = self.all_embed[self.n_entities:, :]\n",
    "        item_emb = self.all_embed[:self.n_entities, :]\n",
    "        return user_emb, item_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac95e8-3eb4-49b0-87c5-c81ab3b2504d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6c4dca-9cc1-415b-a467-f5cea3300013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(train_entity_pairs, start, end, train_user_set, n_items):\n",
    "\n",
    "    def negative_sampling(user_item, train_user_set, n_items):\n",
    "        neg_items = []\n",
    "        for user, _, _ in user_item:\n",
    "            user = int(user)\n",
    "            while True:\n",
    "                neg_item = np.random.randint(low=0, high=n_items, size=1)[0]\n",
    "                if neg_item not in train_user_set[user]:\n",
    "                    break\n",
    "            neg_items.append(neg_item)\n",
    "        return neg_items\n",
    "\n",
    "    feed_dict = {}\n",
    "    entity_pairs = np.array(train_entity_pairs[start:end])\n",
    "    feed_dict['users'] = torch.LongTensor(entity_pairs[:, 0]).to(device)\n",
    "    feed_dict['pos_items'] = torch.LongTensor(entity_pairs[:, 1]).to(device)\n",
    "    feed_dict['neg_items'] = torch.LongTensor(negative_sampling(entity_pairs, train_user_set, n_items)).to(device)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d4353-61de-4154-bfe9-2e2f6c46b1a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a81a3f-94f9-4607-a288-21ed5bde7f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1, keepdim=False) / 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ae6e28-ba7a-401c-ad5b-f79f69db844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc\n",
    "\n",
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = AUC(ground_truth=r, prediction=posterior)\n",
    "    return auc\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n",
    "\n",
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K, user_pos_test))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8414a411-89de-4d0b-8d11-5fb4d7092f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, ground_truth, method=1):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "\n",
    "        Low but correct defination\n",
    "    \"\"\"\n",
    "    GT = set(ground_truth)\n",
    "    if len(GT) > k :\n",
    "        sent_list = [1.0] * k\n",
    "    else:\n",
    "        sent_list = [1.0]*len(GT) + [0.0]*(k-len(GT))\n",
    "    dcg_max = dcg_at_k(sent_list, k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / all_pos_num\n",
    "\n",
    "\n",
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def AUC(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a94f0-e377-4e3f-ae79-8f7500f4d621",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6ab351e-4253-4945-9609-815e27ad14d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, rec, args):\n",
    "    # seed\n",
    "    random.seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    torch.cuda.manual_seed_all(args['seed'])\n",
    "\n",
    "    lst_train_losses = []\n",
    "    lst_rec_losses = []\n",
    "    lst_cor_losses = []\n",
    "    lst_performances = []\n",
    "    recall_list = []\n",
    "    \n",
    "    len_cf =  rec.data.n_cf_train\n",
    "    optimizer  = torch.optim.Adam(model.parameters(), lr=lRate)\n",
    "\n",
    "    # kg_data = rec.data_kg.kg_train_data.to_numpy()\n",
    "    train_cf_data = rec.data.training_data\n",
    "    user_dict = rec.data_kg.user_dict\n",
    "    \n",
    "    for ep in range(maxEpoch):\n",
    "        model.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        cf_losses = []\n",
    "        cor_losses = []\n",
    "        \n",
    "        cf_total_loss = 0\n",
    "        cor_total_loss = 0\n",
    "        \n",
    "        # shuffle(kg_data)\n",
    "        shuffle(train_cf_data)\n",
    "\n",
    "        loss, s, cor_loss = 0,0,0 \n",
    "        train_s_t  = time.time()\n",
    "\n",
    "        with tqdm(total= len_cf // batchSize) as pbar:\n",
    "            while s + batchSize <= len_cf:\n",
    "                batch = get_feed_dict(train_cf_data, s, s+ batchSize, user_dict['train_user_set'], rec.data.n_items)\n",
    "                batch_loss, _, _, batch_cor = model(batch)\n",
    "\n",
    "                batch_loss = batch_loss\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss += batch_loss\n",
    "                cor_loss += batch_cor\n",
    "                s += args['batch_size']\n",
    "                pbar.update(1)\n",
    "\n",
    "                cf_losses.append(batch_loss.item())\n",
    "                cor_losses.append(cor_loss.item()) \n",
    "\n",
    "        cf_loss = np.mean(cf_losses)\n",
    "        cor_loss = np.mean(cor_losses)\n",
    "        train_loss = cf_loss + cor_loss\n",
    "\n",
    "        train_e_t = time.time()\n",
    "        \n",
    "        \"\"\"testing\"\"\"\n",
    "        with torch.no_grad():\n",
    "            user_emb, entity_emb = train_model.calc_cf_embeddings()\n",
    "            item_emb = entity_emb[list(train_model.data_kg.id2i.keys())]\n",
    "            data_ep = rec.fast_evaluation(model, ep, user_emb, item_emb)\n",
    "\n",
    "            # early stopping when cur_best_pre_0 is decreasing for ten successive steps.\n",
    "            cur_recall = float(data_ep[2].split(':')[1])\n",
    "            recall_list.append(cur_recall)\n",
    "            best_recall, should_stop = early_stopping(recall_list, 100)\n",
    "            if should_stop:\n",
    "                break\n",
    "            # \"\"\"save weight\"\"\"\n",
    "            # if ret['recall'][0] == cur_best_pre_0 and args.save:\n",
    "            #     torch.save(model.state_dict(), args['weight_path'])\n",
    "            # else:\n",
    "            # logging.info('training loss at epoch %d: %f' % (epoch, loss.item()))\n",
    "        print('using time %.4f, training loss at epoch %d: %.4f, cor: %.6f' % (train_e_t - train_s_t, ep, loss.item(), cor_loss.item()))\n",
    "        \n",
    "        rec.save_performance_row(ep, data_ep)\n",
    "        rec.save_loss_row([ep, train_loss, cf_loss, cor_loss])\n",
    "    \n",
    "        lst_performances.append(data_ep)\n",
    "        lst_train_losses.append([ep, train_loss]) \n",
    "        lst_rec_losses.append([ep, cf_loss])\n",
    "        lst_cor_losses.append([ep, cor_loss])\n",
    "        \n",
    "    rec.save_loss(lst_train_losses, lst_rec_losses, lst_cor_losses)\n",
    "    rec.save_perfomance_training(lst_performances)\n",
    "    print('early stopping at %d, recall@20:%.4f' % (ep, best_recall))\n",
    "    user_emb, item_emb = rec.best_user_emb, rec.best_item_emb\n",
    "    return user_emb, item_emb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed034219-dbe5-407a-9111-873b9aa94e3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e277edf0-c61c-49cf-ad9b-80f235ae808d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(rec, user_emb, item_emb):\n",
    "    def process_bar(num, total):\n",
    "        rate = float(num) / total\n",
    "        ratenum = int(50 * rate)\n",
    "        r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "        sys.stdout.write(r)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # predict\n",
    "    rec_list = {}\n",
    "    user_count = len(rec.data.test_set)\n",
    "    for i, user in enumerate(rec.data.test_set):\n",
    "        user_id = rec.data_kg.u2id[user]\n",
    "        score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "        candidates = score.cpu().numpy()\n",
    "        \n",
    "        rated_list, li = rec.data.user_rated(user)\n",
    "        for item in rated_list:\n",
    "            candidates[rec.data_kg.i2id[item]] = -10e8\n",
    "        # s_find_k_largest = time.time()\n",
    "        ids, scores = find_k_largest(rec.max_N, candidates)\n",
    "\n",
    "        item_names = [rec.data_kg.id2i[iid] for iid in ids]\n",
    "        rec_list[user] = list(zip(item_names, scores))\n",
    "        if i % 1000 == 0:\n",
    "            process_bar(i, user_count)\n",
    "    process_bar(user_count, user_count)\n",
    "    print('')\n",
    "    rec.evaluate(rec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d52b83-7ba1-4d77-9211-71a8a4932501",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4b7819-e0e2-492b-b18a-9d2d2d105a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'KGIN'\n",
    "config = ModelConf('./conf/' + model + '.conf')\n",
    "lRates = [0.01]\n",
    "lRateKGs = [0.01]\n",
    "lrDecays = [0.9]\n",
    "maxEpochs = [1]\n",
    "batchSizes = [2048]\n",
    "batchSizeKGs = [8192]\n",
    "nLayers = [2]\n",
    "regs = [0.1]\n",
    "regkgs = [ 1e-5]\n",
    "embeddingSizes = [128]\n",
    "datasets = ['lastfm']\n",
    "sim_regularities = [1e-4]\n",
    "context_hopss =  [2]\n",
    "n_factors= [4]\n",
    "node_dropouts= [True]\n",
    "node_dropout_rates= [0.5]\n",
    "inds = ['distance']\n",
    "inverse_r = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a1cd8f-7718-470f-b8f7-a30cf161fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset ='lastfm'\n",
    "training_data = FileIO.load_data_set('./dataset/' + dataset + '/' +config['training.set'], config['model.type'])\n",
    "test_data = FileIO.load_data_set('./dataset/' + dataset + '/'  +config['test.set'], config['model.type'])\n",
    "knowledge_set = FileIO.load_kg_data('./dataset/' + dataset +'/'+ dataset +'.kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "818aafd0-1f19-437c-80f6-70065c678049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the graph ...\n",
      "Begin to load interaction triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################################################################| 69624/69624 [00:00<00:00, 958871.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to load knowledge triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#######################################################################################| 1301288/1301288 [00:05<00:00, 221273.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to build sparse relation matrix ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 37/37 [00:00<00:00, 74.56it/s]\n",
      "/tmp/ipykernel_1725154/3369743934.py:102: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
      "/tmp/ipykernel_1725154/3369743934.py:114: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1).flatten()\n"
     ]
    }
   ],
   "source": [
    "data = Interaction(config, training_data, test_data)\n",
    "data_kg = Knowledge(config, training_data, test_data, knowledge_set, inverse_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "562c19b3-cdea-483e-9bcf-dc36ab74fd36",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter ss_rate is not found in the configuration file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 33/33 [00:26<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [                                                  ]0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.679310 s\n",
      "Measure time: 0.016283 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 1, Hit Ratio:0.00414  |  Precision:0.00255  |  Recall:0.00418  |  NDCG:0.00316\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 1, Hit Ratio:0.00414  |  Precision:0.00255  |  Recall:0.00418  |  NDCG:0.00316\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "using time 26.7208, training loss at epoch 0: 25.6492, cor: 30.217722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 33/33 [00:26<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [                                                  ]0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.721704 s\n",
      "Measure time: 0.015743 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 2, Hit Ratio:0.00504  |  Precision:0.00311  |  Recall:0.0054  |  NDCG:0.00386\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 2, Hit Ratio:0.00504  |  Precision:0.00311  |  Recall:0.0054  |  NDCG:0.00386\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "using time 26.7976, training loss at epoch 1: 20.2047, cor: 30.217722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 33/33 [00:29<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [                                                  ]0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.753691 s\n",
      "Measure time: 0.016889 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 3, Hit Ratio:0.00461  |  Precision:0.00284  |  Recall:0.00498  |  NDCG:0.00376\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 2, Hit Ratio:0.00504  |  Precision:0.00311  |  Recall:0.0054  |  NDCG:0.00386\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "using time 29.4225, training loss at epoch 2: 18.3013, cor: 30.217722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 33/33 [00:29<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [                                                  ]0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.679565 s\n",
      "Measure time: 0.016252 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 4, Hit Ratio:0.00353  |  Precision:0.00218  |  Recall:0.00337  |  NDCG:0.00301\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 2, Hit Ratio:0.00504  |  Precision:0.00311  |  Recall:0.0054  |  NDCG:0.00386\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "using time 29.6755, training loss at epoch 3: 17.6003, cor: 30.217722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 33/33 [00:29<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [                                                  ]0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.714116 s\n",
      "Measure time: 0.015692 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 5, Hit Ratio:0.0034  |  Precision:0.0021  |  Recall:0.00327  |  NDCG:0.00281\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 2, Hit Ratio:0.00504  |  Precision:0.00311  |  Recall:0.0054  |  NDCG:0.00386\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "using time 29.8024, training loss at epoch 4: 17.1772, cor: 30.217722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                                                   | 11/33 [00:10<00:21,  1.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# A_in = TorchGraphInterface.convert_sparse_mat_to_tensor(rec.data_kg.kg_interaction_mat).cuda()\u001b[39;00m\n\u001b[1;32m     42\u001b[0m train_model \u001b[38;5;241m=\u001b[39m KGIN(args, rec, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 43\u001b[0m user_emb, item_emb \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     44\u001b[0m test(rec, user_emb, item_emb)\n",
      "Cell \u001b[0;32mIn[28], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, rec, args)\u001b[0m\n\u001b[1;32m     42\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 44\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n",
      "File \u001b[0;32m~/anaconda3/envs/hungvv/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hungvv/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparameters = [lRates, lRateKGs, lrDecays, maxEpochs, batchSizes, batchSizeKGs, nLayers, regs, embeddingSizes, datasets, sim_regularities, context_hopss, n_factors, node_dropouts, node_dropout_rates, inds]\n",
    "for params in product(*hyperparameters):\n",
    "    lRate, lRateKG, lrDecay, maxEpoch, batchSize, batchSizeKG, nLayer, reg, embeddingSize, dataset, sim_regularity, context_hop, n_factor, node_dropout, node_dropout_rate, ind = params\n",
    "    args = {\n",
    "        'lr': lRate,\n",
    "        'max_epoch': maxEpoch,\n",
    "        'batch_size': batchSize, \n",
    "        'lr_decay': lrDecay,\n",
    "        'dataset': dataset,\n",
    "        'n_layers': nLayer,\n",
    "        'dim': embeddingSize,\n",
    "        'l2': reg,\n",
    "        'sim_regularity': sim_regularity,\n",
    "        'mess_dropout': '[0.1, 0.1, 0.1]',\n",
    "        'mess_dropout_rate': 0.1,\n",
    "        'context_hops': context_hop,\n",
    "        'n_factors': n_factor,\n",
    "        'node_dropout': node_dropout, \n",
    "        'node_dropout_rate': node_dropout_rate,\n",
    "        'ind': ind,\n",
    "        'inverse_r': inverse_r,\n",
    "        'seed': 123,\n",
    "    }\n",
    "\n",
    "    args['output_path'] =  f\"./results/{model}/{dataset}/@{model}-emb:{args['dim']}-bs:{args['batch_size']}-lr:{args['lr']}-n_layers:{args['n_layers']}-reg:{args['l2']}-sim_regularity:{args['sim_regularity']}/\"\n",
    "    if not os.path.exists(args['output_path']):\n",
    "        os.makedirs(args['output_path'])\n",
    "\n",
    "    current_time = strftime(\"%Y-%m-%d\", localtime(time.time()))\n",
    "    file_name =  config['model.name'] + '@' + current_time + '-weight' + '.pth'\n",
    "    weight_path = args['output_path'] + file_name \n",
    "    args['weight_path'] = weight_path\n",
    "    # data\n",
    "    # training_data = FileIO.load_data_set('./dataset/' + dataset + '/' +config['training.set'], config['model.type'])\n",
    "    # test_data = FileIO.load_data_set('./dataset/' + dataset + '/'  +config['test.set'], config['model.type'])\n",
    "    # knowledge_set = FileIO.load_kg_data('./dataset/' + dataset +'/'+ dataset +'.kg')\n",
    "    # data = Interaction(config, training_data, test_data)\n",
    "    # data_kg = Knowledge(config, training_data, test_data, knowledge_set)\n",
    "    # rec \n",
    "    rec = GraphRecommender(config, data, data_kg, knowledge_set, **args)\n",
    "    # A_in = TorchGraphInterface.convert_sparse_mat_to_tensor(rec.data_kg.kg_interaction_mat).cuda()\n",
    "    train_model = KGIN(args, rec, device=device)\n",
    "    user_emb, item_emb = train(train_model, rec, args)   \n",
    "    test(rec, user_emb, item_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hungvv",
   "language": "python",
   "name": "hungvv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
