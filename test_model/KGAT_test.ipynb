{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7fcb158-996c-47b5-ad77-d27bc87611d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiotlab3/anaconda3/envs/hungvv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import collections\n",
    "import scipy.sparse as sp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from util.loss_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e158198-b4ba-4c4d-8b58-542012287406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391743fd-3c68-4398-9800-f631bd674367",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a9e8db-b5e3-4938-8727-5bccebf973b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args \n",
    "        self.data_name = args.data_name \n",
    "        \n",
    "        self.data_dir = os.path.join(args.data_dir, args.data_name)\n",
    "        self.train_file = os.path.join(self.data_dir, 'train.txt')\n",
    "        self.test_file = os.path.join(self.data_dir, 'test.txt')\n",
    "        self.kg_file = os.path.join(self.data_dir, f\"{args.data_name}.kg\")\n",
    "\n",
    "        self.cf_train_data, self.train_user_dict = self.load_cf(self.train_file)\n",
    "        self.cf_test_data, self.test_user_dict = self.load_cf(self.test_file)\n",
    "        self.statistic_cf()\n",
    "\n",
    "    def load_cf(self, filename):\n",
    "        # load user and item from file \n",
    "        user = []\n",
    "        item = []\n",
    "        user_dict = dict()\n",
    "\n",
    "        lines = open(filename, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmp = l.strip()\n",
    "            inter = [i for i in tmp.split('\\t') ]\n",
    "            user_id = int(inter[0])\n",
    "            item_id = int(inter[1])\n",
    "            \n",
    "            user.append(user_id)\n",
    "            item.append(item_id)\n",
    "            \n",
    "            if user_id not in user_dict.keys():\n",
    "                user_dict.update({user_id: [item_id]})\n",
    "            user_dict[user_id].append(item_id)\n",
    "#             if len(inter) > 1:\n",
    "#                 user_id, item_ids = inter[0], inter[1:]\n",
    "#                 item_ids = list(set(item_ids))\n",
    "\n",
    "#                 for item_id in item_ids:\n",
    "#                     user.append(user_id)\n",
    "#                     item.append(item_id)\n",
    "#                 user_dict[user_id] = item_ids\n",
    "\n",
    "        user = np.array(user, dtype=np.int32)\n",
    "        item = np.array(item, dtype=np.int32)\n",
    "        return (user, item), user_dict\n",
    "\n",
    "\n",
    "    def statistic_cf(self):\n",
    "        self.n_users = max(max(self.cf_train_data[0]), max(self.cf_test_data[0])) + 1\n",
    "        self.n_items = max(max(self.cf_train_data[1]), max(self.cf_test_data[1])) + 1\n",
    "        self.n_cf_train = len(self.cf_train_data[0])\n",
    "        self.n_cf_test = len(self.cf_test_data[0])\n",
    "\n",
    "\n",
    "    def load_kg(self, filename):\n",
    "        # load Kg from file \n",
    "        kg_df = pd.read_csv(filename, sep='\\t', header=None, engine='python', skiprows=1, \\\n",
    "                              names= ['head_id:token','relation_id:token','tail_id:token'])\n",
    "        kg_df = kg_df.rename(columns={\n",
    "            'head_id:token': 'h',\n",
    "            'relation_id:token': 'r',\n",
    "            'tail_id:token': 't'\n",
    "        })\n",
    "        return kg_df\n",
    "\n",
    "\n",
    "    def sample_pos_items_for_u(self, user_dict, user_id, n_sample_pos_items):\n",
    "        pos_items = user_dict[user_id]\n",
    "        n_pos_items = len(pos_items)\n",
    "\n",
    "        sample_pos_items = []\n",
    "        while True:\n",
    "            if len(sample_pos_items) == n_sample_pos_items:\n",
    "                break\n",
    "\n",
    "            pos_item_idx = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "            pos_item_id = pos_items[pos_item_idx]\n",
    "            if pos_item_id not in sample_pos_items:\n",
    "                sample_pos_items.append(pos_item_id)\n",
    "        return sample_pos_items\n",
    "\n",
    "\n",
    "    def sample_neg_items_for_u(self, user_dict, user_id, n_sample_neg_items):\n",
    "        pos_items = user_dict[user_id]\n",
    "\n",
    "        sample_neg_items = []\n",
    "        while True:\n",
    "            if len(sample_neg_items) == n_sample_neg_items:\n",
    "                break\n",
    "\n",
    "            neg_item_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "            if neg_item_id not in pos_items and neg_item_id not in sample_neg_items:\n",
    "                sample_neg_items.append(neg_item_id)\n",
    "        return sample_neg_items\n",
    "\n",
    "    def generate_cf_batch(self, user_dict, batch_size):\n",
    "        # sample data for user, item \\\n",
    "        exist_users = list(user_dict.keys())\n",
    "        if batch_size <= len(exist_users):\n",
    "            batch_user = random.sample(exist_users, batch_size)\n",
    "        else:\n",
    "            batch_user = [random.choice(exist_users) for _ in range(batch_size)]\n",
    "        batch_pos_item, batch_neg_item = [], []\n",
    "        for u in batch_user:\n",
    "            batch_pos_item += self.sample_pos_items_for_u(user_dict, u, 1)\n",
    "            batch_neg_item += self.sample_neg_items_for_u(user_dict, u, 1)\n",
    "\n",
    "        batch_user = torch.LongTensor(batch_user)\n",
    "        batch_pos_item = torch.LongTensor(batch_pos_item)\n",
    "        batch_neg_item = torch.LongTensor(batch_neg_item)\n",
    "        return batch_user, batch_pos_item, batch_neg_item\n",
    "\n",
    "    def sample_pos_triples_for_h(self, kg_dict, head, n_sample_pos_triples):\n",
    "        pos_triples = kg_dict[head]\n",
    "        n_pos_triples = len(pos_triples)\n",
    "\n",
    "        sample_relations, sample_pos_tails = [], []\n",
    "        while True:\n",
    "            if len(sample_relations) == n_sample_pos_triples:\n",
    "                break\n",
    "\n",
    "            pos_triple_idx = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "            tail = pos_triples[pos_triple_idx][0]\n",
    "            relation = pos_triples[pos_triple_idx][1]\n",
    "\n",
    "            if relation not in sample_relations and tail not in sample_pos_tails:\n",
    "                sample_relations.append(relation)\n",
    "                sample_pos_tails.append(tail)\n",
    "        return sample_relations, sample_pos_tails\n",
    "\n",
    "\n",
    "    def sample_neg_triples_for_h(self, kg_dict, head, relation, n_sample_neg_triples, highest_neg_idx):\n",
    "        pos_triples = kg_dict[head]\n",
    "\n",
    "        sample_neg_tails = []\n",
    "        while True:\n",
    "            if len(sample_neg_tails) == n_sample_neg_triples:\n",
    "                break\n",
    "\n",
    "            tail = np.random.randint(low=0, high=highest_neg_idx, size=1)[0]\n",
    "            if (tail, relation) not in pos_triples and tail not in sample_neg_tails:\n",
    "                sample_neg_tails.append(tail)\n",
    "        return sample_neg_tails\n",
    "\n",
    "\n",
    "    def generate_kg_batch(self, kg_dict, batch_size, highest_neg_idx):\n",
    "        # sample data for KG \n",
    "        exist_heads = kg_dict.keys()\n",
    "        if batch_size <= len(exist_heads):\n",
    "            batch_head = random.sample(exist_heads, batch_size)\n",
    "        else:\n",
    "            batch_head = [random.choice(exist_heads) for _ in range(batch_size)]\n",
    "\n",
    "        batch_relation, batch_pos_tail, batch_neg_tail = [], [], []\n",
    "        for h in batch_head:\n",
    "            relation, pos_tail = self.sample_pos_triples_for_h(kg_dict, h, 1)\n",
    "            batch_relation += relation\n",
    "            batch_pos_tail += pos_tail\n",
    "\n",
    "            neg_tail = self.sample_neg_triples_for_h(kg_dict, h, relation[0], 1, highest_neg_idx)\n",
    "            batch_neg_tail += neg_tail\n",
    "\n",
    "        batch_head = torch.LongTensor(batch_head)\n",
    "        batch_relation = torch.LongTensor(batch_relation)\n",
    "        batch_pos_tail = torch.LongTensor(batch_pos_tail)\n",
    "        batch_neg_tail = torch.LongTensor(batch_neg_tail)\n",
    "        return batch_head, batch_relation, batch_pos_tail, batch_neg_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211e9660-7e1a-4eb4-b3bd-a14f5b4fbb37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoaderKG(DataLoader):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.cf_batch_size = args.cf_batch_size\n",
    "        self.kg_batch_size = args.kg_batch_size\n",
    "        self.test_batch_size = args.test_batch_size\n",
    "\n",
    "        kg_data = self.load_kg(self.kg_file)\n",
    "        self.construct_data(kg_data)\n",
    "        # self.print_info(logging)\n",
    "\n",
    "        self.laplacian_type = args.laplacian_type\n",
    "        self.create_adjacency_dict()\n",
    "        self.create_laplacian_dict()\n",
    "\n",
    "\n",
    "    def construct_data(self, kg_data):\n",
    "        # add inverse kg data\n",
    "        n_relations = max(kg_data['r']) + 1\n",
    "        inverse_kg_data = kg_data.copy()\n",
    "        inverse_kg_data = inverse_kg_data.rename({'h': 't', 't': 'h'}, axis='columns')\n",
    "        inverse_kg_data['r'] += n_relations\n",
    "        kg_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # re-map user id\n",
    "        kg_data['r'] += 2\n",
    "        self.n_relations = max(kg_data['r']) + 1\n",
    "        self.n_entities = max(max(kg_data['h']), max(kg_data['t'])) + 1\n",
    "        self.n_users_entities = self.n_users + self.n_entities\n",
    "\n",
    "        # self.cf_train_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_train_data[0]))).astype(np.int32), self.cf_train_data[1].astype(np.int32))\n",
    "        # self.cf_test_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_test_data[0]))).astype(np.int32), self.cf_test_data[1].astype(np.int32))\n",
    "\n",
    "        # self.train_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.train_user_dict.items()}\n",
    "        # self.test_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.test_user_dict.items()}\n",
    "\n",
    "        # add interactions to kg data\n",
    "        cf2kg_train_data = pd.DataFrame(np.zeros((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        cf2kg_train_data['h'] = self.cf_train_data[0]\n",
    "        cf2kg_train_data['t'] = self.cf_train_data[1]\n",
    "\n",
    "        inverse_cf2kg_train_data = pd.DataFrame(np.ones((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        inverse_cf2kg_train_data['h'] = self.cf_train_data[1]\n",
    "        inverse_cf2kg_train_data['t'] = self.cf_train_data[0]\n",
    "\n",
    "        self.kg_train_data = pd.concat([kg_data, cf2kg_train_data, inverse_cf2kg_train_data], ignore_index=True)\n",
    "        self.n_kg_train = len(self.kg_train_data)\n",
    "\n",
    "        # construct kg dict\n",
    "        h_list = []\n",
    "        t_list = []\n",
    "        r_list = []\n",
    "\n",
    "        self.train_kg_dict = collections.defaultdict(list)\n",
    "        self.train_relation_dict = collections.defaultdict(list)\n",
    "\n",
    "        for row in self.kg_train_data.iterrows():\n",
    "            h, r, t = row[1]\n",
    "            h_list.append(h)\n",
    "            t_list.append(t)\n",
    "            r_list.append(r)\n",
    "\n",
    "            self.train_kg_dict[h].append((t, r))\n",
    "            self.train_relation_dict[r].append((h, t))\n",
    "\n",
    "        self.h_list = torch.LongTensor(h_list)\n",
    "        self.t_list = torch.LongTensor(t_list)\n",
    "        self.r_list = torch.LongTensor(r_list)\n",
    "\n",
    "\n",
    "    def convert_coo2tensor(self, coo):\n",
    "        values = coo.data\n",
    "        indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = coo.shape\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "\n",
    "    def create_adjacency_dict(self):\n",
    "        # create adjacency matrix from head and tail \n",
    "        self.adjacency_dict = {}\n",
    "        for r, ht_list in self.train_relation_dict.items():\n",
    "            rows = [e[0] for e in ht_list]\n",
    "            cols = [e[1] for e in ht_list]\n",
    "            vals = [1] * len(rows)\n",
    "            adj = sp.coo_matrix((vals, (rows, cols)), shape=(self.n_users_entities, self.n_users_entities))\n",
    "            self.adjacency_dict[r] = adj\n",
    "\n",
    "\n",
    "    def create_laplacian_dict(self):\n",
    "        # create lapalacian adjacencyy matrix\n",
    "        def symmetric_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            norm_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def random_walk_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1.0).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if self.laplacian_type == 'symmetric':\n",
    "            norm_lap_func = symmetric_norm_lap\n",
    "        elif self.laplacian_type == 'random-walk':\n",
    "            norm_lap_func = random_walk_norm_lap\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.laplacian_dict = {}\n",
    "        for r, adj in self.adjacency_dict.items():\n",
    "            self.laplacian_dict[r] = norm_lap_func(adj)\n",
    "\n",
    "        A_in = sum(self.laplacian_dict.values())\n",
    "        self.A_in = self.convert_coo2tensor(A_in.tocoo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad1f9c-5ae0-42bb-92d2-740cdbf1e1b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6554f172-e655-4244-bda2-8f210691d884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_kgat_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run KGAT.\")\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=2019,\n",
    "                        help='Random seed.')\n",
    "\n",
    "    parser.add_argument('--data_name', nargs='?', default='lastfm',\n",
    "                        help='Choose a dataset from {ml-1m, lastfm, amazon-book}')\n",
    "    parser.add_argument('--data_dir', nargs='?', default='dataset/',\n",
    "                        help='Input data path.')\n",
    "    \n",
    "    parser.add_argument('--use_pretrain', type=int, default=0,\n",
    "                    help='0: No pretrain, 1: Pretrain with the learned embeddings, 2: Pretrain with stored model.')\n",
    "\n",
    "    parser.add_argument('--cf_batch_size', type=int, default=2048,\n",
    "                        help='CF batch size.')\n",
    "    parser.add_argument('--kg_batch_size', type=int, default=8192,\n",
    "                        help='KG batch size.')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=2048,\n",
    "                        help='Test batch size (the user number to test every batch).')\n",
    "\n",
    "    parser.add_argument('--embed_dim', type=int, default=16,\n",
    "                        help='User / entity Embedding size.')\n",
    "    parser.add_argument('--relation_dim', type=int, default=16,\n",
    "                        help='Relation Embedding size.')\n",
    "\n",
    "    parser.add_argument('--laplacian_type', type=str, default='random-walk',\n",
    "                        help='Specify the type of the adjacency (laplacian) matrix from {symmetric, random-walk}.')\n",
    "    parser.add_argument('--aggregation_type', type=str, default='bi-interaction',\n",
    "                        help='Specify the type of the aggregation layer from {gcn, graphsage, bi-interaction}.')\n",
    "    parser.add_argument('--conv_dim_list', nargs='?', default='[64, 32, 16]',\n",
    "                        help='Output sizes of every aggregation layer.')\n",
    "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1, 0.1]',\n",
    "                        help='Dropout probability w.r.t. message dropout for each deep layer. 0: no dropout.')\n",
    "\n",
    "    parser.add_argument('--kg_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating KG l2 loss.')\n",
    "    parser.add_argument('--cf_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating CF l2 loss.')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--n_epoch', type=int, default=100,\n",
    "                        help='Number of epoch.')\n",
    "    parser.add_argument('--stopping_steps', type=int, default=10,\n",
    "                        help='Number of epoch for early stopping')\n",
    "\n",
    "    parser.add_argument('--cf_print_every', type=int, default=20,\n",
    "                        help='Iter interval of printing CF loss.')\n",
    "    parser.add_argument('--kg_print_every', type=int, default=20,\n",
    "                        help='Iter interval of printing KG loss.')\n",
    "    parser.add_argument('--evaluate_every', type=int, default=1,\n",
    "                        help='Epoch interval of evaluating CF.')\n",
    "\n",
    "    parser.add_argument('--Ks', nargs='?', default='[10, 20]',\n",
    "                        help='Calculate metric@K when evaluating.')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    save_dir = 'results/KGAT/{}/embed-dim{}_relation-dim{}_{}_{}_{}_lr{}/'.format(\n",
    "        args.data_name, args.embed_dim, args.relation_dim, args.laplacian_type, args.aggregation_type,\n",
    "        '-'.join([str(i) for i in eval(args.conv_dim_list)]), args.lr)\n",
    "    args.save_dir = save_dir\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5954e8-51b9-457e-8ff1-1efa6fc2e40e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f35ac4d-86a5-460e-914d-ab79b5fe0c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1, keepdim=False) / 2.)\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, dropout, aggregator_type):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.aggregator_type = aggregator_type\n",
    "\n",
    "        self.message_dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            self.linear = nn.Linear(self.in_dim, self.out_dim)       # W in Equation (6)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            self.linear = nn.Linear(self.in_dim * 2, self.out_dim)   # W in Equation (7)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            self.linear1 = nn.Linear(self.in_dim, self.out_dim)      # W1 in Equation (8)\n",
    "            self.linear2 = nn.Linear(self.in_dim, self.out_dim)      # W2 in Equation (8)\n",
    "            nn.init.xavier_uniform_(self.linear1.weight)\n",
    "            nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def forward(self, ego_embeddings, A_in):\n",
    "        \"\"\"\n",
    "        ego_embeddings:  (n_users + n_entities, in_dim)\n",
    "        A_in:            (n_users + n_entities, n_users + n_entities), torch.sparse.FloatTensor\n",
    "        \"\"\"\n",
    "        # Equation (3)\n",
    "        side_embeddings = torch.matmul(A_in, ego_embeddings)\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            # Equation (6) & (9)\n",
    "            embeddings = ego_embeddings + side_embeddings\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            # Equation (7) & (9)\n",
    "            embeddings = torch.cat([ego_embeddings, side_embeddings], dim=1)\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            # Equation (8) & (9)\n",
    "            sum_embeddings = self.activation(self.linear1(ego_embeddings + side_embeddings))\n",
    "            bi_embeddings = self.activation(self.linear2(ego_embeddings * side_embeddings))\n",
    "            embeddings = bi_embeddings + sum_embeddings\n",
    "\n",
    "        embeddings = self.message_dropout(embeddings)           # (n_users + n_entities, out_dim)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678d4e89-6c94-4c49-ba09-66a7998568d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KGAT(nn.Module):\n",
    "\n",
    "    def __init__(self, args,\n",
    "                 n_users, n_entities, n_relations, A_in=None,\n",
    "                 user_pre_embed=None, item_pre_embed=None):\n",
    "\n",
    "        super(KGAT, self).__init__()\n",
    "        self.use_pretrain = args.use_pretrain\n",
    "\n",
    "        self.n_users = n_users\n",
    "        self.n_entities = n_entities\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.relation_dim = args.relation_dim\n",
    "\n",
    "        self.aggregation_type = args.aggregation_type\n",
    "        self.conv_dim_list = [args.embed_dim] + eval(args.conv_dim_list)\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.n_layers = len(eval(args.conv_dim_list))\n",
    "\n",
    "        self.kg_l2loss_lambda = args.kg_l2loss_lambda\n",
    "        self.cf_l2loss_lambda = args.cf_l2loss_lambda\n",
    "\n",
    "        self.entity_user_embed = nn.Embedding(self.n_entities + self.n_users, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(self.n_relations, self.relation_dim)\n",
    "        self.trans_M = nn.Parameter(torch.Tensor(self.n_relations, self.embed_dim, self.relation_dim))\n",
    "\n",
    "        if (self.use_pretrain == 1) and (user_pre_embed is not None) and (item_pre_embed is not None):\n",
    "            other_entity_embed = nn.Parameter(torch.Tensor(self.n_entities - item_pre_embed.shape[0], self.embed_dim))\n",
    "            nn.init.xavier_uniform_(other_entity_embed)\n",
    "            entity_user_embed = torch.cat([item_pre_embed, other_entity_embed, user_pre_embed], dim=0)\n",
    "            self.entity_user_embed.weight = nn.Parameter(entity_user_embed)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.entity_user_embed.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_embed.weight)\n",
    "        nn.init.xavier_uniform_(self.trans_M)\n",
    "\n",
    "        self.aggregator_layers = nn.ModuleList()\n",
    "        for k in range(self.n_layers):\n",
    "            self.aggregator_layers.append(Aggregator(self.conv_dim_list[k], self.conv_dim_list[k + 1], self.mess_dropout[k], self.aggregation_type))\n",
    "\n",
    "        self.A_in = nn.Parameter(torch.sparse.FloatTensor(self.n_users + self.n_entities, self.n_users + self.n_entities))\n",
    "        if A_in is not None:\n",
    "            self.A_in.data = A_in\n",
    "        self.A_in.requires_grad = False\n",
    "\n",
    "    def calc_cf_embeddings(self):\n",
    "        ego_embed = self.entity_user_embed.weight\n",
    "        all_embed = [ego_embed]\n",
    "\n",
    "        for idx, layer in enumerate(self.aggregator_layers):\n",
    "            ego_embed = layer(ego_embed, self.A_in)\n",
    "            norm_embed = F.normalize(ego_embed, p=2, dim=1)\n",
    "            all_embed.append(norm_embed)\n",
    "\n",
    "        # Equation (11)\n",
    "        all_embed = torch.cat(all_embed, dim=1)         # (n_users + n_entities, concat_dim)\n",
    "        return all_embed\n",
    "\n",
    "\n",
    "    def calc_cf_loss(self, user_ids, item_pos_ids, item_neg_ids):\n",
    "        \"\"\"\n",
    "        user_ids:       (cf_batch_size)\n",
    "        item_pos_ids:   (cf_batch_size)\n",
    "        item_neg_ids:   (cf_batch_size)\n",
    "        \"\"\"\n",
    "        all_embed = self.calc_cf_embeddings()                       # (n_users + n_entities, concat_dim)\n",
    "        user_embed = all_embed[user_ids]                            # (cf_batch_size, concat_dim)\n",
    "        item_pos_embed = all_embed[item_pos_ids]                    # (cf_batch_size, concat_dim)\n",
    "        item_neg_embed = all_embed[item_neg_ids]                    # (cf_batch_size, concat_dim)\n",
    "\n",
    "        # Equation (12)\n",
    "        pos_score = torch.sum(user_embed * item_pos_embed, dim=1)   # (cf_batch_size)\n",
    "        neg_score = torch.sum(user_embed * item_neg_embed, dim=1)   # (cf_batch_size)\n",
    "\n",
    "        # Equation (13)\n",
    "        # cf_loss = F.softplus(neg_score - pos_score)\n",
    "        cf_loss = (-1.0) * F.logsigmoid(pos_score - neg_score)\n",
    "        cf_loss = torch.mean(cf_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(user_embed) + _L2_loss_mean(item_pos_embed) + _L2_loss_mean(item_neg_embed)\n",
    "        loss = cf_loss + self.cf_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def calc_kg_loss(self, h, r, pos_t, neg_t):\n",
    "        \"\"\"\n",
    "        h:      (kg_batch_size)\n",
    "        r:      (kg_batch_size)\n",
    "        pos_t:  (kg_batch_size)\n",
    "        neg_t:  (kg_batch_size)\n",
    "        \"\"\"\n",
    "        r_embed = self.relation_embed(r)                                                # (kg_batch_size, relation_dim)\n",
    "        W_r = self.trans_M[r]                                                           # (kg_batch_size, embed_dim, relation_dim)\n",
    "\n",
    "        h_embed = self.entity_user_embed(h)                                             # (kg_batch_size, embed_dim)\n",
    "        pos_t_embed = self.entity_user_embed(pos_t)                                     # (kg_batch_size, embed_dim)\n",
    "        neg_t_embed = self.entity_user_embed(neg_t)                                     # (kg_batch_size, embed_dim)\n",
    "\n",
    "        r_mul_h = torch.bmm(h_embed.unsqueeze(1), W_r).squeeze(1)                       # (kg_batch_size, relation_dim)\n",
    "        r_mul_pos_t = torch.bmm(pos_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "        r_mul_neg_t = torch.bmm(neg_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "\n",
    "        # Equation (1)\n",
    "        pos_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_pos_t, 2), dim=1)     # (kg_batch_size)\n",
    "        neg_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_neg_t, 2), dim=1)     # (kg_batch_size)\n",
    "\n",
    "        # Equation (2)\n",
    "        # kg_loss = F.softplus(pos_score - neg_score)\n",
    "        kg_loss = (-1.0) * F.logsigmoid(neg_score - pos_score)\n",
    "        kg_loss = torch.mean(kg_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(r_mul_h) + _L2_loss_mean(r_embed) + _L2_loss_mean(r_mul_pos_t) + _L2_loss_mean(r_mul_neg_t)\n",
    "        loss = kg_loss + self.kg_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update_attention_batch(self, h_list, t_list, r_idx):\n",
    "        r_embed = self.relation_embed.weight[r_idx]\n",
    "        W_r = self.trans_M[r_idx]\n",
    "\n",
    "        h_embed = self.entity_user_embed.weight[h_list]\n",
    "        t_embed = self.entity_user_embed.weight[t_list]\n",
    "\n",
    "        # Equation (4)\n",
    "        r_mul_h = torch.matmul(h_embed, W_r)\n",
    "        r_mul_t = torch.matmul(t_embed, W_r)\n",
    "        v_list = torch.sum(r_mul_t * torch.tanh(r_mul_h + r_embed), dim=1)\n",
    "        return v_list\n",
    "\n",
    "    def update_attention(self, h_list, t_list, r_list, relations):\n",
    "        device = self.A_in.device\n",
    "\n",
    "        rows = []\n",
    "        cols = []\n",
    "        values = []\n",
    "\n",
    "        for r_idx in relations:\n",
    "            index_list = torch.where(r_list == r_idx)\n",
    "            batch_h_list = h_list[index_list]\n",
    "            batch_t_list = t_list[index_list]\n",
    "\n",
    "            batch_v_list = self.update_attention_batch(batch_h_list, batch_t_list, r_idx)\n",
    "            rows.append(batch_h_list)\n",
    "            cols.append(batch_t_list)\n",
    "            values.append(batch_v_list)\n",
    "\n",
    "        rows = torch.cat(rows)\n",
    "        cols = torch.cat(cols)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        indices = torch.stack([rows, cols])\n",
    "        shape = self.A_in.shape\n",
    "        A_in = torch.sparse.FloatTensor(indices, values, torch.Size(shape))\n",
    "\n",
    "        # Equation (5)\n",
    "        A_in = torch.sparse.softmax(A_in.cpu(), dim=1)\n",
    "        self.A_in.data = A_in.to(device)\n",
    "\n",
    "\n",
    "    def calc_score(self, user_ids, item_ids):\n",
    "        \"\"\"\n",
    "        user_ids:  (n_users)\n",
    "        item_ids:  (n_items)\n",
    "        \"\"\"\n",
    "        all_embed = self.calc_cf_embeddings()           # (n_users + n_entities, concat_dim)\n",
    "        user_embed = all_embed[user_ids]                # (n_users, concat_dim)\n",
    "        item_embed = all_embed[item_ids]                # (n_items, concat_dim)\n",
    "\n",
    "        # Equation (12)\n",
    "        cf_score = torch.matmul(user_embed, item_embed.transpose(0, 1))    # (n_users, n_items)\n",
    "        return cf_score\n",
    "\n",
    "\n",
    "    def forward(self, *input, mode):\n",
    "        if mode == 'train_cf':\n",
    "            return self.calc_cf_loss(*input)\n",
    "        if mode == 'train_kg':\n",
    "            return self.calc_kg_loss(*input)\n",
    "        if mode == 'update_att':\n",
    "            return self.update_attention(*input)\n",
    "        if mode == 'predict':\n",
    "            return self.calc_score(*input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957a073-20f1-4d00-878a-6c35a11e76dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c3146b-a763-481b-85aa-ec9e55fb839c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss, mean_squared_error\n",
    "\n",
    "\n",
    "def calc_recall(rank, ground_truth, k):\n",
    "    \"\"\"\n",
    "    calculate recall of one example\n",
    "    \"\"\"\n",
    "    return len(set(rank[:k]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "def hit_ratio_at_k(hit, k):\n",
    "    '''\n",
    "    calc hit ratio of one example \n",
    "    '''\n",
    "    hit_k = np.asarray(hit)[:k]\n",
    "    return np.sum(hit_k) / np.sum(hit)\n",
    "\n",
    "def hit_ratio_at_k_batch(hits, k):\n",
    "    res = hits[:, :k].sum(axis=1) / hits.sum(axis=1)  \n",
    "    return res \n",
    "\n",
    "def precision_at_k(hit, k):\n",
    "    \"\"\"\n",
    "    calculate Precision@k\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)[:k]\n",
    "    return np.mean(hit)\n",
    "\n",
    "\n",
    "def precision_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate Precision@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    res = hits[:, :k].mean(axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def average_precision(hit, cut):\n",
    "    \"\"\"\n",
    "    calculate average precision (area under PR curve)\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)\n",
    "    precisions = [precision_at_k(hit, k + 1) for k in range(cut) if len(hit) >= k]\n",
    "    if not precisions:\n",
    "        return 0.\n",
    "    return np.sum(precisions) / float(min(cut, np.sum(hit)))\n",
    "\n",
    "\n",
    "def dcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    calculate discounted cumulative gain (dcg)\n",
    "    rel: list, element is positive real values, can be binary\n",
    "    \"\"\"\n",
    "    rel = np.asfarray(rel)[:k]\n",
    "    dcg = np.sum((2 ** rel - 1) / np.log2(np.arange(2, rel.size + 2)))\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    calculate normalized discounted cumulative gain (ndcg)\n",
    "    rel: list, element is positive real values, can be binary\n",
    "    \"\"\"\n",
    "    idcg = dcg_at_k(sorted(rel, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(rel, k) / idcg\n",
    "\n",
    "\n",
    "def ndcg_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate NDCG@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    hits_k = hits[:, :k]\n",
    "    dcg = np.sum((2 ** hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\n",
    "    sorted_hits_k = np.flip(np.sort(hits), axis=1)[:, :k]\n",
    "    idcg = np.sum((2 ** sorted_hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\n",
    "    idcg[idcg == 0] = np.inf\n",
    "    ndcg = (dcg / idcg)\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def recall_at_k(hit, k, all_pos_num):\n",
    "    \"\"\"\n",
    "    calculate Recall@k\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asfarray(hit)[:k]\n",
    "    return np.sum(hit) / all_pos_num\n",
    "\n",
    "\n",
    "def recall_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate Recall@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    res = (hits[:, :k].sum(axis=1) / hits.sum(axis=1))\n",
    "    return res\n",
    "\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def calc_auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res\n",
    "\n",
    "\n",
    "def logloss(ground_truth, prediction):\n",
    "    logloss = log_loss(np.asarray(ground_truth), np.asarray(prediction))\n",
    "    return logloss\n",
    "\n",
    "def calc_metrics_at_k(cf_scores, train_user_dict, test_user_dict, user_ids, item_ids, Ks):\n",
    "    \"\"\"\n",
    "    cf_scores: (n_users, n_items)\n",
    "    \"\"\"\n",
    "    test_pos_item_binary = np.zeros([len(user_ids), len(item_ids)], dtype=np.float32)\n",
    "    for idx, u in enumerate(user_ids):\n",
    "        try:\n",
    "            train_pos_item_list = train_user_dict[u]\n",
    "            test_pos_item_list = test_user_dict[u]\n",
    "            cf_scores[idx][train_pos_item_list] = -np.inf\n",
    "            test_pos_item_binary[idx][test_pos_item_list] = 1\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        _, rank_indices = torch.sort(cf_scores.to(device), descending=True)    # try to speed up the sorting process\n",
    "    except:\n",
    "        _, rank_indices = torch.sort(cf_scores, descending=True)\n",
    "    rank_indices = rank_indices.cpu()\n",
    "\n",
    "    binary_hit = []\n",
    "    for i in range(len(user_ids)):\n",
    "        binary_hit.append(test_pos_item_binary[i][rank_indices[i]])\n",
    "    binary_hit = np.array(binary_hit, dtype=np.float32)\n",
    "\n",
    "    metrics_dict = {}\n",
    "    for k in Ks:\n",
    "        metrics_dict[k] = {}\n",
    "        metrics_dict[k]['hit'] = hit_ratio_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['precision'] = precision_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['recall']    = recall_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['ndcg']      = ndcg_at_k_batch(binary_hit, k)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27eafc-49ae-415a-a6e4-f2f7d865fab5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735ef40b-7d9d-4054-ae7c-9cb359d526ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def early_stopping(recall_list, stopping_steps):\n",
    "    best_recall = max(recall_list)\n",
    "    best_step = recall_list.index(best_recall)\n",
    "    if len(recall_list) - best_step - 1 >= stopping_steps:\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "    return best_recall, should_stop\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, current_epoch, last_best_epoch=None):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(current_epoch))\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'epoch': current_epoch}, model_state_file)\n",
    "\n",
    "    if last_best_epoch is not None and current_epoch != last_best_epoch:\n",
    "        old_model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(last_best_epoch))\n",
    "        if os.path.exists(old_model_state_file):\n",
    "            os.system('rm {}'.format(old_model_state_file))\n",
    "\n",
    "def convert_sparse_mat_to_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    i = torch.LongTensor([coo.row, coo.col])\n",
    "    v = torch.from_numpy(coo.data).float()\n",
    "    return torch.sparse.FloatTensor(i, v, coo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf447b-769f-4958-98e2-20c4a54f53cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62b15b4-1670-404e-bbf6-cd1d37640bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['hit', 'precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_scores = batch_scores.cpu()\n",
    "            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            cf_scores.append(batch_scores.numpy())\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    cf_scores = np.concatenate(cf_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return cf_scores, metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "524fc10d-032d-4e15-8877-fa7dea867ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss(anchor_emb, pos_emb, neg_emb, batch_size, reg):\n",
    "    calc_reg_loss = EmbLoss()\n",
    "    rec_loss = bpr_loss(anchor_emb, pos_emb, neg_emb)\n",
    "    reg_loss = reg * calc_reg_loss(anchor_emb, pos_emb, neg_emb) / batch_size\n",
    "    return rec_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaec0b6d-e59c-4265-a38f-1e54721305c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, args):    \n",
    "    cf_optimizer = optim.Adam(model.parameters(), lr = args.lr)\n",
    "    kg_optimizer = optim.Adam(model.parameters(), lr = args.lr)    \n",
    "    \n",
    "    # initialize metrics\n",
    "    best_epoch = -1\n",
    "    best_recall = 0\n",
    "    reg = 0.1\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    epoch_list = []\n",
    "    metrics_list = {k: {'hit': [],'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        time0 = time()\n",
    "        model.train()\n",
    "\n",
    "        # train cf\n",
    "        time1 = time()\n",
    "        cf_total_loss = 0\n",
    "        n_cf_batch = data.n_cf_train // data.cf_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_cf_batch + 1):\n",
    "            time2 = time()\n",
    "            cf_batch_user, cf_batch_pos_item, cf_batch_neg_item = data.generate_cf_batch(data.train_user_dict, data.cf_batch_size)\n",
    "            cf_batch_user = cf_batch_user.to(device)\n",
    "            cf_batch_pos_item = cf_batch_pos_item.to(device)\n",
    "            cf_batch_neg_item = cf_batch_neg_item.to(device) \n",
    "            cf_batch_loss = model(cf_batch_user, cf_batch_pos_item, cf_batch_neg_item, mode='train_cf')\n",
    "\n",
    "            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n",
    "                print('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_cf_batch))\n",
    "                return \n",
    "            \n",
    "            cf_batch_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_total_loss += cf_batch_loss.item()\n",
    "\n",
    "            if (iter % args.cf_print_every) == 0:\n",
    "                print('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_cf_batch, time() - time2, cf_batch_loss.item(), cf_total_loss / iter))\n",
    "        print('CF Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_cf_batch, time() - time1, cf_total_loss / n_cf_batch))\n",
    "\n",
    "        # train kg\n",
    "        time3 = time()\n",
    "        kg_total_loss = 0\n",
    "        n_kg_batch = data.n_kg_train // data.kg_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_kg_batch + 1):\n",
    "            time4 = time()\n",
    "            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = data.generate_kg_batch(data.train_kg_dict, data.kg_batch_size, data.n_users_entities)\n",
    "            kg_batch_head = kg_batch_head.to(device)\n",
    "            kg_batch_relation = kg_batch_relation.to(device)\n",
    "            kg_batch_pos_tail = kg_batch_pos_tail.to(device)\n",
    "            kg_batch_neg_tail = kg_batch_neg_tail.to(device)\n",
    "\n",
    "            kg_batch_loss = model(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail, mode='train_kg')\n",
    "\n",
    "            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n",
    "                print('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_kg_batch))\n",
    "                return \n",
    "\n",
    "            kg_batch_loss.backward()\n",
    "            kg_optimizer.step()\n",
    "            kg_optimizer.zero_grad()\n",
    "            kg_total_loss += kg_batch_loss.item()\n",
    "\n",
    "            if (iter % args.kg_print_every) == 0:\n",
    "                print('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_kg_batch, time() - time4, kg_batch_loss.item(), kg_total_loss / iter))\n",
    "        print('KG Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_kg_batch, time() - time3, kg_total_loss / n_kg_batch))\n",
    "\n",
    "        # update attention\n",
    "        time5 = time()\n",
    "        h_list = data.h_list.to(device)\n",
    "        t_list = data.t_list.to(device)\n",
    "        r_list = data.r_list.to(device)\n",
    "        relations = list(data.laplacian_dict.keys())\n",
    "        model(h_list, t_list, r_list, relations, mode='update_att')\n",
    "        print('Update Attention: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time5))\n",
    "\n",
    "        print('CF + KG Training: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time0))\n",
    "\n",
    "        # evaluate cf\n",
    "        if (epoch % args.evaluate_every) == 0 or epoch == args.n_epoch:\n",
    "            time6 = time()\n",
    "            _, metrics_dict = evaluate(model, data, Ks, device)\n",
    "            print('CF Evaluation: Epoch {:04d} | Total Time {:.1f}s | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "                epoch, time() - time6, metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "            epoch_list.append(epoch)\n",
    "            for k in Ks:\n",
    "                for m in ['hit', 'precision', 'recall', 'ndcg']:\n",
    "                    metrics_list[k][m].append(metrics_dict[k][m])\n",
    "            best_recall, should_stop = early_stopping(metrics_list[k_min]['recall'], args.stopping_steps)\n",
    "\n",
    "            if should_stop:\n",
    "                break\n",
    "\n",
    "            if metrics_list[k_min]['recall'].index(best_recall) == len(epoch_list) - 1:\n",
    "                save_model(model, args.save_dir, epoch, best_epoch)\n",
    "                print('Save model on epoch {:04d}!'.format(epoch))\n",
    "                best_epoch = epoch\n",
    "\n",
    "    # save metrics\n",
    "    metrics_df = [epoch_list]\n",
    "    metrics_cols = ['epoch_idx']\n",
    "    for k in Ks:\n",
    "        for m in ['hit', 'precision', 'recall', 'ndcg']:\n",
    "            metrics_df.append(metrics_list[k][m])\n",
    "            metrics_cols.append('{}@{}'.format(m, k))\n",
    "            \n",
    "    metrics_df = pd.DataFrame(metrics_df).transpose()\n",
    "    metrics_df.columns = metrics_cols\n",
    "    metrics_df.to_csv(args.save_dir + '/metrics.csv', sep='\\t', index=False)\n",
    "\n",
    "    # print best metrics\n",
    "    best_metrics = metrics_df.loc[metrics_df['epoch_idx'] == best_epoch].iloc[0].to_dict()\n",
    "    print('Best CF Evaluation: Epoch {:04d} | Hit [{:.4f}, {:.4f}], Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        int(best_metrics['epoch_idx']), best_metrics['hit@{}'.format(k_min)], best_metrics['hit@{}'.format(k_max)], best_metrics['precision@{}'.format(k_min)], best_metrics['precision@{}'.format(k_max)], best_metrics['recall@{}'.format(k_min)], best_metrics['recall@{}'.format(k_max)], best_metrics['ndcg@{}'.format(k_min)], best_metrics['ndcg@{}'.format(k_max)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac08798-895e-4ef2-b096-5d31d6bf3eec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f4a5fe-5907-4f28-8822-735f34d1ee00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = parse_kgat_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfaa9e8c-a492-401f-bb62-bf0197d4f8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1333202/398779490.py:108: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n"
     ]
    }
   ],
   "source": [
    "data = DataLoaderKG(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f28f4eba-3438-4a98-b18b-874af0be6f35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([452934, 454034, 453848, ..., 452012, 453409, 453493], dtype=int32),\n",
       " array([ 488,  193,  163, ...,  535, 1043, 1180], dtype=int32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cf_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "270bd880-5e10-4e39-a8bf-3870e8fc8e66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_pre_embed, item_pre_embed = None, None\n",
    "model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54460eeb-b097-435d-9de6-991e7806ee17",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Training: Epoch 0001 Iter 0020 / 0034 | Time 1.0s | Iter Loss 0.6370 | Iter Mean Loss 0.6621\n",
      "CF Training: Epoch 0001 Total Iter 0034 | Total Time 35.7s | Iter Mean Loss 0.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1333202/426732001.py:150: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  batch_head = random.sample(exist_heads, batch_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG Training: Epoch 0001 Iter 0020 / 0181 | Time 0.2s | Iter Loss 0.6783 | Iter Mean Loss 0.6870\n",
      "KG Training: Epoch 0001 Iter 0040 / 0181 | Time 0.2s | Iter Loss 0.6264 | Iter Mean Loss 0.6713\n",
      "KG Training: Epoch 0001 Iter 0060 / 0181 | Time 0.2s | Iter Loss 0.5199 | Iter Mean Loss 0.6391\n",
      "KG Training: Epoch 0001 Iter 0080 / 0181 | Time 0.2s | Iter Loss 0.3883 | Iter Mean Loss 0.5921\n",
      "KG Training: Epoch 0001 Iter 0100 / 0181 | Time 0.2s | Iter Loss 0.2807 | Iter Mean Loss 0.5395\n",
      "KG Training: Epoch 0001 Iter 0120 / 0181 | Time 0.2s | Iter Loss 0.2016 | Iter Mean Loss 0.4890\n",
      "KG Training: Epoch 0001 Iter 0140 / 0181 | Time 0.2s | Iter Loss 0.1515 | Iter Mean Loss 0.4441\n",
      "KG Training: Epoch 0001 Iter 0160 / 0181 | Time 0.2s | Iter Loss 0.1155 | Iter Mean Loss 0.4051\n",
      "KG Training: Epoch 0001 Iter 0180 / 0181 | Time 0.2s | Iter Loss 0.0916 | Iter Mean Loss 0.3715\n",
      "KG Training: Epoch 0001 Total Iter 0181 | Total Time 28.4s | Iter Mean Loss 0.3699\n",
      "Update Attention: Epoch 0001 | Total Time 0.2s\n",
      "CF + KG Training: Epoch 0001 | Total Time 64.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Iteration:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_1333202/1896999341.py:20: RuntimeWarning: invalid value encountered in divide\n",
      "  res = hits[:, :k].sum(axis=1) / hits.sum(axis=1)\n",
      "/tmp/ipykernel_1333202/1896999341.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  res = (hits[:, :k].sum(axis=1) / hits.sum(axis=1))\n",
      "Evaluating Iteration: 100%|| 1/1 [00:01<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Evaluation: Epoch 0001 | Total Time 1.7s | Precision [0.0502, 0.0427], Recall [nan, nan], NDCG [0.0550, 0.0641]\n",
      "Save model on epoch 0001!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, args)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR (CF Training): Epoch \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m Iter \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m Loss is nan.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, \u001b[38;5;28miter\u001b[39m, n_cf_batch))\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \n\u001b[0;32m---> 38\u001b[0m \u001b[43mcf_batch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m cf_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m cf_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/hungvv/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hungvv/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f8c8c-e39a-444d-96d9-bafd8fa407af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
